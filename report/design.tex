\section{Design}
\label{section:design}
To develop a climate and activity based outfit suggestion tool, the problem has been broken into parts.
At its core is a machine learning algorithm along with a datastore for the training data. Wrapping that is
a query system and eventually a mobile app for training the system. Additionally, an outfit modeller and persistence
engine will be implemented base on experience from evaluating the system by hand.
In this section, the envisioned parts of the system are presented. Note, this is the vision for the system as a whole and
not all components have been implemented for this report.

\subsection{Learning Core}
Firstly, Support Vector Machine (SVM) \cite{SVM} was chosen as the machine learning method for implementing an outfit predictor.
Justification for this choice is provided in Section \ref{section:mlsvm}. Some other methods which were considered are
denoted in \ref{section:mloptions}. The learning core was implemented as a Node.js RESTful API to make it easy for a
Mobile App or Web UI to consume on behalf of users of Gear Smarts. It has the additional benefit of being very easy to
script against, something that made evaluation of the tool much more approachable.
The API is described below in Section \ref{section:mlapi}.

\subsubsection{Justification for SVM}
\label{section:mlsvm}
Support Vector Machines were chosen over alternative machine learning methods for three main reasons.

Firstly, SVMs are built for classification problems and can be configured in a number of ways (such as linear vs.
nonlinear classifying) to make it extensible to a wider variety of problems than other methods might allow. This is
ideal for GearSmarts since we are exploring the problem space as much as solving it. The flexibility of SVMs will
help us react to changing or perhaps surprising discoveries.

The second reason for using SVMs is there is already an easy-to-use Node.js implementation library, \texttt{node-svm}
\cite{Github:nodesvm}. Since the details of implementing SVM are abstracted away, all we need concern ourselves with in
the GearSmarts API is normalizing and namespacing the datasets provided by consumers. Even better, \texttt{node-svm}
simply uses LIBSVM \cite{lib:libsvm} under the hood, which has been ported to or exposed in most major languages. The
widespread availability of this library means GearSmarts could be easily ported as-is to any of those languages.

Thirdly, the author already had experience with popular machine learning methods like Aritificial Neural Networks, and
Decision Trees. Support vector machines offer an opportunity to learn a new method and compare it to experiences with
other methods.

\subsubsection{Collecting Data}
In lieu of having a UI for users to train the system, a Google Form questionairre (included in Appendix
\ref{appendix:comfort_form}) was created for
friends and family to contribute datapoints. The response dataset includes about 50 datapoints (Appendix \ref{appendix:comfort_responses}).

Since the responses were provided in a clear-text and ``human'' format, they were reformatted by hand into a CSV format
starting with a class (``good'', ``cold'', ``hott'') and a locale descriptor, followed by activity, demographic, and
outfit features. To overcome the limited dataset size, the polled responses were duplicated and altered to produce ``obvious''
comfort classifications. For example if a person is hot in an outfit, adding more layers to it will still be hott.
Likewise, a ``common sense'' approach was used to fill in the opposite comfort class. For example, it's safe to say anybody
would be hot in three coats on a 70-degree day, and that anybody would be cold in a T-shirt on a 5-degree day.
More details on the data normalization process are discussed in Section \ref{subsection:preprocessing}

Regardless of the data collected for the outfit-specific problem, GearSmarts capabilities can be estimated by evaluating
it with published classification datasets, discussed in Section \ref{subsection:publicdatasets}. By assuring that the API
is capable of learning once the trained dataset is large enough, the target of outfit comfort classification
can be mastered more gradually over time with use of the system.

\subsubsection{Machine Learning RESTful API}
\label{section:mlapi}
In order to make Gear Smarts as extensible and usable as possible, the machine learning core was implemented in Node.js
as a RESTful server, exposing two key routes describe below.

\begin{description}
    \item{\texttt{/v1/machinelearning/:namespace/train/:classification}} - Train the classifier on a single feature vector
    \item{\texttt{/v1/machinelearning/:namespace/classify}} - Classify a single feature vector
\end{description}

Both routes take a \texttt{:namespace} argument, which is simply an extensibility mechanism allowing consumers to train
many distinct datasets. For example, consumers training the \texttt{candy} namespace would not affect consumers training
the \texttt{fruits} namespace.

Additionally, each route expects a \texttt{POST} body or \texttt{GET} query containing an Array of features to
train or classify under the \texttt{features} property. Interaction with a GearSmarts server is as simple as
making an HTTP call. An example of a training call is shown presently:

\begin{lstlisting}
http://localhost:8080/v1/ml/outfits/train/good
?features="['ski','groomers','meantempi=40',
'snow_pants','base_top','base_bottom','shell']"
\end{lstlisting}



In order to normalize the dataset, the server puts features into a dictionary of known features and uses their index
in the list for the SVM. The same combination of dictionary and indexing is used for the classifications to. This approach
was taken because the SVM library requires integer inputs. Examples of the dictionaries are shown in table
\ref{table:dictionary} and table \ref{table:datarow}. As an additional noe,
trained data rows are stored in the API to more easily reason over the produced SVM. This storage currently lives in
memory only, but would eventually be moved to a database of some sort for persistence in a productized GearSmarts.
This data store also would make it easy to query and search outfits in use towards the marketing goals mentioned in
Section \ref{section:market}.

\begin{table}
    \begin{tabular}{lll}
        \hline
        \multicolumn{3}{c}{Classifications} \\
        \hline
        0 & 1 & 2 \\
        hott & good & cold \\
        \hline
        \hline
    \end{tabular}
    \begin{tabular}{lll}
        \multicolumn{3}{c}{Features} \\
        \hline
        0 & 1 & 2 \\
        meantempi=35 & snowpants & ... \\
        \hline
    \end{tabular}
    \caption{The SVM prefers integer classifications and features, so GearSmarts stores an Array of known classes and features,
    and uses their indexes in the SVM}
    \label{table:dictionary}
\end{table}

\begin{table}
    \begin{tabular}{llllr}
        \hline
        \multicolumn{5}{c}{Trained Data} \\
        \multicolumn{4}{c}{Features} & Class \\
        \hline
        ski     & downCoat  & helmet      & temp=25  & 0 \\
        iceFish & downParka  & woolHat      & temp=20  & 2 \\
        hike    & tshirt & temp=70  & & 1 \\
        \hline
    \end{tabular}
    \caption{The dataset is stored as a table with an Array of feature names and Array of Arrays of feature values. Note that rows
    do not need to have consistent feature vector sizes.}
    \label{table:datarow}
\end{table}

Along with the main classifier routes, GearSmarts also supports the querying of weather data using Wunderground \cite{wunderground}.
The route is accessed with a simple HTTP GET including a location and date, shown below. To reduce usage of Wunderground's limited
free tier API, all queries are cached on the filesystem. This has the additional benefit of making the usage of the weather info
easy to use in evaluation scripts.

\begin{description}
    \item{\texttt{/v1/weather?date=12/31/2014\&region=CO\&city=Colorado Springs}} - Get weather for a given date and place
\end{description}

\subsection{Problem Modeling}
Before using a machine learning system, the problem needed to be modeled properly. It can be broken down into three
fundamental parts, described below.

\begin{description}
  \item{Outfit:} Outfits can either be modeled individually or as a summation of individual articles (potentially introducing
  a knapsack problem\footnote{The knapsack problem seeks to optimize a value by searching combinations of a set. In Gear Smarts,
  we might represent an outfit with a KPI that should closely match a KPI produced by a function of the weather. Meanwhile, each
  article of clothing in a wardrobe could have a KPI suggesting how warm it is. To properly match the knapsack problem, we might
  introduce a mobility metric for each article.}).
  Additionally, they can be represented categorically (e.g. vest or full-zip and
  fleece, down, and cotton) or by derived characteristics (e.g. thermal retention, mobility, wind-resistance).
  \item{Weather:} Weather should likely be characterized in terms of temperature, humidity, wind, pressure,
  cloud cover, and precipitation.
  \item{Comfort:} Intuitively, there are three basic comfort levels relevant to Gear Smart: too cold, comfortable, and too hot.
\end{description}

Although a KPI could be used, to make the system more extensible, GearSmarts models the problem as a set of
articles of clothing, along with a true/false value for each. The UI will submit a list of articles the user wore (or
plans to wear) to the API, which maintains a dictionary (described above \ref{table:dictionary}) of known articles. The
API then normalizes the provided articles against the dictionary, producing a key/value object where the values are
simply \texttt{true} or \texttt{false} denoting if each is present.

Weather data is queried through the GearSmarts API as a key/value object of important
weather attributes in exchange for the consumer-provided location and date. The resulting key/value object should be
normalized by the consumer before being merged with outfit features to construct a feature vector. More details about
the normalization process used for this report are discussed in \ref{section:evaluation}. As is typical in
machine learning, the technique used to normalize the features has an enormous impact on the effectiveness of the
classifier.

The comfort level would be selected from a static list of options (``hot,'' ``cold,'' and ``good'') by the user when
training the system. When the UI tries to make an outfit suggestion, it would search the available set of articles of
clothing (a ``wardrobe'') combined with the known weather features, querying the machine learning \texttt{/classify}
route to find a subset of the wardrobe with a ``good'' classification.
